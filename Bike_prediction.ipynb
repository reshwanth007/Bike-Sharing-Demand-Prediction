{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoejRSv5U4rVjnnqurjKeA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyGy7EU8teZE",
        "outputId": "8b480e48-4717-4e60-bdb6-a478d14b8c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Loaded hour.csv shape=(17379, 17)\n",
            "Data after FE shape: (17355, 24)\n",
            "\n",
            "--- EDA plots saving to artifacts/ ---\n",
            "EDA plots saved.\n",
            "Train rows: 13884, Test rows: 3471\n",
            "Numeric features: ['temp', 'atemp', 'hum', 'windspeed', 'hr_sin', 'hr_cos', 'doy_sin', 'doy_cos', 'lag_1', 'lag_24', 'roll_mean_3', 'roll_mean_24']\n",
            "Categorical features: ['season', 'mnth', 'hr', 'weekday', 'workingday', 'holiday', 'weathersit', 'yr', 'is_weekend']\n",
            "\n",
            "Tuning Ridge ...\n",
            " -> best params: {'model__alpha': np.float64(0.001)}\n",
            "\n",
            "Tuning Lasso ...\n",
            " -> best params: {'model__alpha': np.float64(0.0035564803062231283)}\n",
            "\n",
            "Tuning HistGB ...\n",
            " -> best params: {'model__min_samples_leaf': 5, 'model__max_leaf_nodes': 15, 'model__max_iter': 400, 'model__learning_rate': 0.1}\n",
            "\n",
            "Model results:\n",
            "Ridge: Test RMSE=75.706, R2=0.882\n",
            "Lasso: Test RMSE=75.703, R2=0.882\n",
            "HistGB: Test RMSE=45.619, R2=0.957\n",
            "\n",
            "Best by Test RMSE: HistGB\n",
            "Computing permutation importance for HistGB ...\n",
            "Permutation importance saved for HistGB.\n",
            "\n",
            "All artifacts saved to: /content/artifacts\n"
          ]
        }
      ],
      "source": [
        "# Advanced Bike-Sharing Demand â€” Research-Grade End-to-End (Colab)\n",
        "# Paste entire cell into Colab and run.\n",
        "# Features: robust FE, lag features, TimeSeries CV, RandomizedSearch, HistGradientBoostingRegressor, permutation importance.\n",
        "\n",
        "import os, io, zipfile, urllib.request, json, warnings, math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "\n",
        "import joblib\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# ---------------------------\n",
        "# Config\n",
        "# ---------------------------\n",
        "DATASET = \"hour\"   # \"hour\" or \"day\"\n",
        "TEST_SIZE = 0.2    # chronological holdout\n",
        "RANDOM_STATE = 42\n",
        "ARTIFACTS = \"artifacts\"\n",
        "USE_LOG_TARGET = False   # set True to model log1p(cnt)\n",
        "LAGS = [1, 24]           # lag features to add (1, 24 hours)\n",
        "ROLL_WINDOWS = [3, 24]   # rolling windows (in hours) if hour data\n",
        "\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities & Eval dataclass\n",
        "# ---------------------------\n",
        "def ensure_dir(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "def rmse(y, yhat):\n",
        "    return math.sqrt(mean_squared_error(y, yhat))\n",
        "\n",
        "@dataclass\n",
        "class EvalResult:\n",
        "    name: str\n",
        "    train_rmse: float\n",
        "    train_mae: float\n",
        "    train_r2: float\n",
        "    test_rmse: float\n",
        "    test_mae: float\n",
        "    test_r2: float\n",
        "\n",
        "    def as_dict(self):\n",
        "        return {\n",
        "            \"train_rmse\": self.train_rmse,\n",
        "            \"train_mae\": self.train_mae,\n",
        "            \"train_r2\": self.train_r2,\n",
        "            \"test_rmse\": self.test_rmse,\n",
        "            \"test_mae\": self.test_mae,\n",
        "            \"test_r2\": self.test_r2,\n",
        "        }\n",
        "\n",
        "# ---------------------------\n",
        "# Data download / load\n",
        "# ---------------------------\n",
        "UCI_ZIP = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\"\n",
        "HOUR_CSV = \"hour.csv\"\n",
        "DAY_CSV = \"day.csv\"\n",
        "\n",
        "def download_and_extract():\n",
        "    try:\n",
        "        print(\"Downloading UCI dataset...\")\n",
        "        with urllib.request.urlopen(UCI_ZIP, timeout=60) as r:\n",
        "            data = r.read()\n",
        "        z = zipfile.ZipFile(io.BytesIO(data))\n",
        "        for fn in [HOUR_CSV, DAY_CSV]:\n",
        "            if fn in z.namelist():\n",
        "                print(f\"Extracting {fn}\")\n",
        "                with z.open(fn) as src, open(fn, \"wb\") as dst:\n",
        "                    dst.write(src.read())\n",
        "        print(\"Done.\")\n",
        "    except Exception as e:\n",
        "        print(\"Download failed:\", e)\n",
        "\n",
        "def load_dataset(kind=\"hour\"):\n",
        "    assert kind in (\"hour\",\"day\")\n",
        "    target = HOUR_CSV if kind==\"hour\" else DAY_CSV\n",
        "    if not os.path.exists(target):\n",
        "        download_and_extract()\n",
        "    if os.path.exists(target):\n",
        "        df = pd.read_csv(target)\n",
        "        print(f\"Loaded {target} shape={df.shape}\")\n",
        "        return df\n",
        "    # fallback synthetic\n",
        "    print(\"Falling back to synthetic dataset (for demo).\")\n",
        "    if kind==\"hour\":\n",
        "        dates = pd.date_range(\"2018-01-01\", periods=24*365, freq=\"H\")\n",
        "        n = len(dates)\n",
        "        df = pd.DataFrame({\n",
        "            \"dteday\": dates.date,\n",
        "            \"hr\": dates.hour,\n",
        "            \"weekday\": dates.weekday,\n",
        "            \"mnth\": dates.month,\n",
        "            \"season\": ((dates.month % 12) // 3) + 1,\n",
        "            \"workingday\": ((dates.weekday < 5) * 1),\n",
        "            \"holiday\": 0,\n",
        "            \"weathersit\": np.random.choice([1,2,3], size=n, p=[0.7,0.25,0.05]),\n",
        "            \"temp\": np.clip(0.5 + 0.4*np.sin(2*np.pi*dates.dayofyear/365 - np.pi/2) + np.random.normal(0,0.05,n), 0, 1),\n",
        "            \"atemp\": np.random.rand(n),\n",
        "            \"hum\": np.clip(0.6 + 0.2*np.sin(2*np.pi*dates.dayofyear/365) + np.random.normal(0,0.1,n), 0, 1),\n",
        "            \"windspeed\": np.clip(0.3 + np.random.normal(0,0.1,n), 0, 1),\n",
        "        })\n",
        "        # create cnt with peaks\n",
        "        hour = df[\"hr\"].values\n",
        "        commute = 80*np.exp(-0.5*((hour-8)/2.2)**2) + 90*np.exp(-0.5*((hour-18)/2.5)**2)\n",
        "        base = 120 + 200*df[\"workingday\"]\n",
        "        noise = np.random.normal(0,40,len(df))\n",
        "        df[\"cnt\"] = np.clip(base + commute + 100*(df[\"temp\"]-0.3) + noise, 0, None).astype(int)\n",
        "        return df\n",
        "    else:\n",
        "        dates = pd.date_range(\"2018-01-01\", periods=730, freq=\"D\")\n",
        "        n = len(dates)\n",
        "        df = pd.DataFrame({\n",
        "            \"dteday\": dates.date,\n",
        "            \"weekday\": dates.weekday,\n",
        "            \"mnth\": dates.month,\n",
        "            \"season\": ((dates.month % 12) // 3) + 1,\n",
        "            \"workingday\": ((dates.weekday < 5) * 1),\n",
        "            \"holiday\": 0,\n",
        "            \"weathersit\": np.random.choice([1,2,3], size=n, p=[0.7,0.25,0.05]),\n",
        "            \"temp\": np.clip(0.5 + 0.4*np.sin(2*np.pi*dates.dayofyear/365 - np.pi/2) + np.random.normal(0,0.05,n), 0, 1),\n",
        "            \"atemp\": np.random.rand(n),\n",
        "            \"hum\": np.clip(0.6 + 0.2*np.sin(2*np.pi*dates.dayofyear/365) + np.random.normal(0,0.1,n), 0, 1),\n",
        "            \"windspeed\": np.clip(0.3 + np.random.normal(0,0.1,n), 0, 1),\n",
        "        })\n",
        "        base = 2500 + 800*df[\"workingday\"]\n",
        "        noise = np.random.normal(0,300,n)\n",
        "        df[\"cnt\"] = np.clip(base + 1200*(df[\"season\"].isin([2,3])) + 1500*(df[\"temp\"]-0.3) + noise, 0, None).astype(int)\n",
        "        return df\n",
        "\n",
        "# ---------------------------\n",
        "# Feature engineering transformer (create lags inside pipeline isn't trivial,\n",
        "# so we create lags before pipeline as they require access to ordered rows)\n",
        "# ---------------------------\n",
        "def create_time_features(df, kind=\"hour\"):\n",
        "    df = df.copy()\n",
        "    if \"dteday\" in df.columns:\n",
        "        df[\"dteday\"] = pd.to_datetime(df[\"dteday\"])\n",
        "    if kind==\"hour\":\n",
        "        # ensure hr exists (hourly dataset)\n",
        "        if \"hr\" not in df.columns:\n",
        "            # maybe hour provided via datetime index\n",
        "            df[\"hr\"] = df[\"dteday\"].dt.hour\n",
        "        df[\"timestamp\"] = df[\"dteday\"] + pd.to_timedelta(df[\"hr\"], unit=\"h\")\n",
        "    else:\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"dteday\"])\n",
        "    df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "    # cyclical encodings\n",
        "    if \"hr\" in df.columns:\n",
        "        df[\"hr_sin\"] = np.sin(2*np.pi*df[\"hr\"]/24)\n",
        "        df[\"hr_cos\"] = np.cos(2*np.pi*df[\"hr\"]/24)\n",
        "    doy = df[\"timestamp\"].dt.dayofyear\n",
        "    df[\"doy_sin\"] = np.sin(2*np.pi*doy/365)\n",
        "    df[\"doy_cos\"] = np.cos(2*np.pi*doy/365)\n",
        "    df[\"is_weekend\"] = df[\"weekday\"].isin([5,6]).astype(int) if \"weekday\" in df.columns else 0\n",
        "    # drop leakage\n",
        "    for c in [\"casual\",\"registered\",\"instant\"]:\n",
        "        if c in df.columns:\n",
        "            df.drop(columns=[c], inplace=True)\n",
        "    return df\n",
        "\n",
        "def add_lag_and_roll_features(df, lags=[1,24], roll_windows=[3,24], target=\"cnt\"):\n",
        "    df = df.copy()\n",
        "    for lag in lags:\n",
        "        df[f\"lag_{lag}\"] = df[target].shift(lag)\n",
        "    for w in roll_windows:\n",
        "        df[f\"roll_mean_{w}\"] = df[target].shift(1).rolling(window=w, min_periods=1).mean()\n",
        "    # After lag/roll creation we drop rows with NaN (start of series)\n",
        "    df = df.dropna().reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "# ---------------------------\n",
        "# Preprocessing & columns\n",
        "# ---------------------------\n",
        "def get_feature_lists(df):\n",
        "    numeric_candidates = [\"temp\",\"atemp\",\"hum\",\"windspeed\",\"hr_sin\",\"hr_cos\",\"doy_sin\",\"doy_cos\"] + [f\"lag_{l}\" for l in LAGS] + [f\"roll_mean_{w}\" for w in ROLL_WINDOWS]\n",
        "    numeric = [c for c in numeric_candidates if c in df.columns]\n",
        "    categorical_candidates = [\"season\",\"mnth\",\"hr\",\"weekday\",\"workingday\",\"holiday\",\"weathersit\",\"yr\",\"is_weekend\",\"is_peak_hour\"]\n",
        "    categorical = [c for c in categorical_candidates if c in df.columns]\n",
        "    return numeric, categorical\n",
        "\n",
        "def build_preprocessor(numeric, categorical):\n",
        "    num_pipe = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "    cat_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    pre = ColumnTransformer([\n",
        "        (\"num\", num_pipe, numeric),\n",
        "        (\"cat\", cat_pipe, categorical)\n",
        "    ], remainder=\"drop\")\n",
        "    return pre\n",
        "\n",
        "# ---------------------------\n",
        "# Modeling & tuning\n",
        "# ---------------------------\n",
        "def fit_and_tune(X_train, y_train, pre):\n",
        "    # Models to tune (Ridge/Lasso/HistGB)\n",
        "    estimators = {\n",
        "        \"Ridge\": Ridge(random_state=RANDOM_STATE),\n",
        "        \"Lasso\": Lasso(random_state=RANDOM_STATE, max_iter=5000),\n",
        "        \"HistGB\": HistGradientBoostingRegressor(random_state=RANDOM_STATE)\n",
        "    }\n",
        "\n",
        "    # Pipelines\n",
        "    pipes = {k: Pipeline([(\"pre\", pre), (\"model\", v)]) for k,v in estimators.items()}\n",
        "\n",
        "    # Hyperparameter search spaces (Randomized)\n",
        "    param_distributions = {\n",
        "        \"Ridge\": {\n",
        "            \"model__alpha\": np.logspace(-3,3,50)\n",
        "        },\n",
        "        \"Lasso\": {\n",
        "            \"model__alpha\": np.logspace(-4,0,50)\n",
        "        },\n",
        "        \"HistGB\": {\n",
        "            \"model__learning_rate\": [0.01,0.03,0.05,0.1],\n",
        "            \"model__max_iter\": [100,200,400],\n",
        "            \"model__max_leaf_nodes\": [15,31,63,127],\n",
        "            \"model__min_samples_leaf\": [5,10,20]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    tscv = TimeSeriesSplit(n_splits=5)\n",
        "    fitted = {}\n",
        "    results = {}\n",
        "\n",
        "    for name, pipe in pipes.items():\n",
        "        print(f\"\\nTuning {name} ...\")\n",
        "        params = param_distributions.get(name, {})\n",
        "        if params:\n",
        "            search = RandomizedSearchCV(pipe, params, n_iter=30, cv=tscv, scoring=\"neg_root_mean_squared_error\", n_jobs=-1, random_state=RANDOM_STATE, verbose=0)\n",
        "            search.fit(X_train, y_train)\n",
        "            best = search.best_estimator_\n",
        "            print(f\" -> best params: {search.best_params_}\")\n",
        "        else:\n",
        "            pipe.fit(X_train, y_train)\n",
        "            best = pipe\n",
        "        # compute train predictions (on training portion) and store\n",
        "        fitted[name] = best\n",
        "    return fitted\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation helpers\n",
        "# ---------------------------\n",
        "def evaluate_model(pipe, Xtr, ytr, Xte, yte, use_log=False):\n",
        "    # predictions\n",
        "    if use_log:\n",
        "        yp_tr = np.expm1(pipe.predict(Xtr))\n",
        "        yp_te = np.expm1(pipe.predict(Xte))\n",
        "    else:\n",
        "        yp_tr = pipe.predict(Xtr)\n",
        "        yp_te = pipe.predict(Xte)\n",
        "    res = EvalResult(\n",
        "        name = getattr(pipe.named_steps[\"model\"], \"__class__\", type(pipe)).__name__,\n",
        "        train_rmse = rmse(ytr, yp_tr),\n",
        "        train_mae = mean_absolute_error(ytr, yp_tr),\n",
        "        train_r2 = r2_score(ytr, yp_tr),\n",
        "        test_rmse = rmse(yte, yp_te),\n",
        "        test_mae = mean_absolute_error(yte, yp_te),\n",
        "        test_r2 = r2_score(yte, yp_te),\n",
        "    )\n",
        "    return res, yp_tr, yp_te\n",
        "\n",
        "# ---------------------------\n",
        "# Plotting functions (EDA + diagnostics)\n",
        "# ---------------------------\n",
        "def eda_plots(df):\n",
        "    ensure_dir(ARTIFACTS)\n",
        "    print(\"\\n--- EDA plots saving to artifacts/ ---\")\n",
        "    plt.figure(figsize=(8,5))\n",
        "    sns.histplot(df['cnt'], bins=40, kde=True)\n",
        "    plt.title(\"Distribution of bike rentals (cnt)\")\n",
        "    plt.savefig(os.path.join(ARTIFACTS, \"eda_cnt_dist.png\")); plt.close()\n",
        "\n",
        "    if \"hr\" in df.columns:\n",
        "        plt.figure(figsize=(10,6))\n",
        "        df_group = df.groupby(\"hr\")[\"cnt\"].mean().reset_index()\n",
        "        sns.lineplot(data=df_group, x=\"hr\", y=\"cnt\")\n",
        "        plt.title(\"Average rentals by hour\")\n",
        "        plt.savefig(os.path.join(ARTIFACTS, \"eda_avg_by_hour.png\")); plt.close()\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.boxplot(x=\"season\", y=\"cnt\", data=df)\n",
        "    plt.title(\"Rentals by season\")\n",
        "    plt.savefig(os.path.join(ARTIFACTS, \"eda_by_season.png\")); plt.close()\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.barplot(x=\"workingday\", y=\"cnt\", data=df, estimator=np.mean)\n",
        "    plt.title(\"Average rentals: workingday vs non-workingday\")\n",
        "    plt.savefig(os.path.join(ARTIFACTS, \"eda_workingday.png\")); plt.close()\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(df.corr(), cmap=\"coolwarm\", center=0)\n",
        "    plt.title(\"Correlation heatmap\")\n",
        "    plt.savefig(os.path.join(ARTIFACTS, \"eda_corr.png\")); plt.close()\n",
        "    print(\"EDA plots saved.\")\n",
        "\n",
        "def plot_predictions_and_residuals(ts_test, y_test, y_pred, model_name):\n",
        "    ensure_dir(ARTIFACTS)\n",
        "    # scatter\n",
        "    plt.figure(figsize=(7,6))\n",
        "    plt.scatter(y_test, y_pred, alpha=0.4)\n",
        "    mx = max(y_test.max(), np.nanmax(y_pred))\n",
        "    plt.plot([0,mx],[0,mx],'r--')\n",
        "    plt.xlabel(\"True cnt\"); plt.ylabel(\"Predicted cnt\"); plt.title(f\"{model_name} True vs Predicted (scatter)\")\n",
        "    plt.savefig(os.path.join(ARTIFACTS, f\"{model_name}_true_vs_pred_scatter.png\")); plt.close()\n",
        "\n",
        "    # residual histogram\n",
        "    resid = y_test - y_pred\n",
        "    plt.figure(figsize=(7,5))\n",
        "    sns.histplot(resid, bins=40, kde=True)\n",
        "    plt.title(f\"{model_name} Residual distribution\")\n",
        "    plt.savefig(os.path.join(ARTIFACTS, f\"{model_name}_resid_hist.png\")); plt.close()\n",
        "\n",
        "    # time series sample (first 500 points)\n",
        "    nplot = min(500, len(y_test))\n",
        "    plt.figure(figsize=(12,5))\n",
        "    plt.plot(ts_test.iloc[:nplot], y_test[:nplot], label=\"True\")\n",
        "    plt.plot(ts_test.iloc[:nplot], y_pred[:nplot], label=\"Pred\")\n",
        "    plt.legend(); plt.title(f\"{model_name} Actual vs Pred (sample {nplot})\")\n",
        "    plt.xticks(rotation=30)\n",
        "    plt.savefig(os.path.join(ARTIFACTS, f\"{model_name}_actual_vs_pred_timeseries.png\")); plt.close()\n",
        "\n",
        "def save_metrics(all_results: Dict[str, EvalResult]):\n",
        "    ensure_dir(ARTIFACTS)\n",
        "    out = {k:v.as_dict() for k,v in all_results.items()}\n",
        "    with open(os.path.join(ARTIFACTS, \"metrics_all.json\"), \"w\") as f:\n",
        "        json.dump(out, f, indent=2)\n",
        "\n",
        "# ---------------------------\n",
        "# Permutation importance (post-fit)\n",
        "# ---------------------------\n",
        "def compute_and_save_permutation(pipe, X_test, y_test, numeric, categorical, model_name):\n",
        "    from sklearn.inspection import permutation_importance\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import os\n",
        "\n",
        "    print(f\"Computing permutation importance for {model_name} ...\")\n",
        "\n",
        "    r = permutation_importance(\n",
        "        pipe, X_test, y_test,\n",
        "        n_repeats=10, random_state=42, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # 1) Locate the preprocessor step robustly\n",
        "    pre = pipe.named_steps.get(\"pre\") or pipe.named_steps.get(\"preprocessor\")\n",
        "    if pre is None:\n",
        "        raise KeyError(\"Could not find preprocessing step. Expected 'pre' or 'preprocessor' in pipeline named_steps.\")\n",
        "\n",
        "    # 2) Numeric feature names (as passed in)\n",
        "    num_names = list(numeric)\n",
        "\n",
        "    # 3) Recover OHE-expanded categorical names robustly\n",
        "    cat_names_expanded = []\n",
        "    if \"cat\" in pre.named_transformers_:\n",
        "        cat_trans = pre.named_transformers_[\"cat\"]\n",
        "        # Try to find OneHotEncoder inside the categorical pipeline\n",
        "        ohe = None\n",
        "        if hasattr(cat_trans, \"named_steps\") and \"ohe\" in cat_trans.named_steps:\n",
        "            ohe = cat_trans.named_steps[\"ohe\"]\n",
        "        else:\n",
        "            # Search for OneHotEncoder instance\n",
        "            for step_name, step_obj in getattr(cat_trans, \"named_steps\", {}).items():\n",
        "                from sklearn.preprocessing import OneHotEncoder\n",
        "                if isinstance(step_obj, OneHotEncoder):\n",
        "                    ohe = step_obj\n",
        "                    break\n",
        "        if ohe is not None:\n",
        "            try:\n",
        "                cat_names_expanded = list(ohe.get_feature_names_out(categorical))\n",
        "            except Exception:\n",
        "                # Fallback if feature names cannot be extracted\n",
        "                cat_names_expanded = [f\"cat_{i}\" for i in range(len(ohe.categories_))]\n",
        "        else:\n",
        "            # No OHE found; fallback to original categorical names\n",
        "            cat_names_expanded = list(categorical)\n",
        "    else:\n",
        "        # No categorical transformer present\n",
        "        cat_names_expanded = []\n",
        "\n",
        "    feat_names = num_names + cat_names_expanded\n",
        "\n",
        "    # Align to permutation importance length (safety)\n",
        "    n_importances = len(r.importances_mean)\n",
        "    if len(feat_names) != n_importances:\n",
        "        # Fallback: pad or trim to match lengths\n",
        "        if len(feat_names) < n_importances:\n",
        "            feat_names = feat_names + [f\"feat_{i}\" for i in range(n_importances - len(feat_names))]\n",
        "        else:\n",
        "            feat_names = feat_names[:n_importances]\n",
        "\n",
        "    # Save to CSV\n",
        "    dfimp = pd.DataFrame({\n",
        "        \"feature\": feat_names,\n",
        "        \"importance_mean\": r.importances_mean,\n",
        "        \"importance_std\": r.importances_std\n",
        "    }).sort_values(\"importance_mean\", ascending=False)\n",
        "\n",
        "    os.makedirs(ARTIFACTS, exist_ok=True)\n",
        "    dfimp.to_csv(os.path.join(ARTIFACTS, f\"{model_name}_permutation_importance.csv\"), index=False)\n",
        "\n",
        "    # Save plot\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    topk = min(20, len(dfimp))\n",
        "    sns.barplot(data=dfimp.head(topk), x=\"importance_mean\", y=\"feature\")\n",
        "    plt.title(f\"Top {topk} Features - {model_name} Permutation Importance\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(ARTIFACTS, f\"{model_name}_permutation_importance.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Permutation importance saved for {model_name}.\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Main run\n",
        "# ---------------------------\n",
        "def main():\n",
        "    ensure_dir(ARTIFACTS)\n",
        "    print(\"Loading dataset...\")\n",
        "    raw = load_dataset(DATASET)\n",
        "    df = create_time_features(raw, kind=DATASET)\n",
        "\n",
        "    # Add lag & rolling features (may drop early rows)\n",
        "    if DATASET==\"hour\":\n",
        "        df = add_lag_and_roll_features(df, lags=LAGS, roll_windows=ROLL_WINDOWS)\n",
        "    else:\n",
        "        # daily: use only lag 1\n",
        "        df = add_lag_and_roll_features(df, lags=[1], roll_windows=[7])\n",
        "\n",
        "    print(\"Data after FE shape:\", df.shape)\n",
        "    eda_plots(df)\n",
        "\n",
        "    # Optionally transform target\n",
        "    if USE_LOG_TARGET:\n",
        "        df[\"y\"] = np.log1p(df[\"cnt\"])\n",
        "    else:\n",
        "        df[\"y\"] = df[\"cnt\"]\n",
        "\n",
        "    # Chronological split\n",
        "    n = len(df)\n",
        "    split_idx = int((1-TEST_SIZE) * n)\n",
        "    train = df.iloc[:split_idx].copy()\n",
        "    test  = df.iloc[split_idx:].copy()\n",
        "    print(f\"Train rows: {len(train)}, Test rows: {len(test)}\")\n",
        "\n",
        "    numeric, categorical = get_feature_lists(df)\n",
        "    X_train = train[numeric + categorical]\n",
        "    y_train = train[\"y\"].values if USE_LOG_TARGET else train[\"cnt\"].values\n",
        "    X_test  = test[numeric + categorical]\n",
        "    y_test  = test[\"y\"].values if USE_LOG_TARGET else test[\"cnt\"].values\n",
        "\n",
        "    print(\"Numeric features:\", numeric)\n",
        "    print(\"Categorical features:\", categorical)\n",
        "\n",
        "    pre = build_preprocessor(numeric, categorical)\n",
        "    fitted = fit_and_tune(X_train, y_train, pre)\n",
        "\n",
        "    # Evaluate all fitted models and pick best by test RMSE\n",
        "    all_results = {}\n",
        "    preds = {}\n",
        "    for name, pipe in fitted.items():\n",
        "        res, ytr_pred, yte_pred = evaluate_model(pipe, X_train, y_train, X_test, y_test, use_log=USE_LOG_TARGET)\n",
        "        all_results[name] = res\n",
        "        preds[name] = yte_pred\n",
        "        # save predictions & diagnostic plots per model\n",
        "        # use test timestamp for plotting\n",
        "        plot_predictions_and_residuals(test[\"timestamp\"], y_test, yte_pred, name)\n",
        "\n",
        "    # pick best\n",
        "    best_name = min(all_results.keys(), key=lambda k: all_results[k].test_rmse)\n",
        "    print(\"\\nModel results:\")\n",
        "    for k,v in all_results.items():\n",
        "        print(f\"{k}: Test RMSE={v.test_rmse:.3f}, R2={v.test_r2:.3f}\")\n",
        "\n",
        "    print(f\"\\nBest by Test RMSE: {best_name}\")\n",
        "    save_metrics(all_results)\n",
        "\n",
        "    # Save test predictions for best model\n",
        "    best_pipe = fitted[best_name]\n",
        "    best_pred = preds[best_name]\n",
        "    out_df = pd.DataFrame({\n",
        "        \"timestamp\": test[\"timestamp\"].values,\n",
        "        \"y_true\": test[\"cnt\"].values,\n",
        "        \"y_pred\": np.round(best_pred, 3)\n",
        "    })\n",
        "    out_df.to_csv(os.path.join(ARTIFACTS, \"test_predictions_best.csv\"), index=False)\n",
        "    # Save model\n",
        "    joblib.dump(best_pipe, os.path.join(ARTIFACTS, \"best_pipe.joblib\"))\n",
        "\n",
        "    # Permutation importance for best model\n",
        "    compute_and_save_permutation(best_pipe, X_test, test[\"cnt\"].values, numeric, categorical, best_name)\n",
        "\n",
        "    print(\"\\nAll artifacts saved to:\", os.path.abspath(ARTIFACTS))\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}